{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6Mo5xyEXnP6nUGqpW8Zbu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Aminata Thioune**\n","\n","\n","1. **Human Judged**\n","\n","2. **LLM judged**\n","  * **Deep Eval**\n","  * **MLFLOW**\n","  * **Test on Modal** (en cours) -> AdaptLLM\n","\n","3. **Standard metrics**\n","  * LLAMA -> quantization\n","  * Mistral -> quantization\n"],"metadata":{"id":"rwU8cR6UZ9Qq"}},{"cell_type":"markdown","source":["#**DeepEval_GPT**"],"metadata":{"id":"P_13P7IbmEYm"}},{"cell_type":"markdown","source":["DeepEval est un framework open source d'évaluation basé sur un LLMJudged (gpt-4o par défault) pour évaluer les LLMs en quantifiant leurs performances et leur fiabilité.\n","\n","DeepEval utilise diverses métriques pour fournir une évaluation des modèles, telles que :\n","\n","\n","*  G-Eval\n","*  Summarization\n","*  Hallucination\n","*  Faithfulness\n","*  Contextual Relevancy\n","*  Answer Relevancy\n","*  Contextual Recall\n","*  Contextual Precision\n","*  RAGAS\n","*  Bias\n","*  Toxicity\n","*  Knowledge Retention\n","\n"],"metadata":{"id":"w7UbxC0IojWa"}},{"cell_type":"code","source":["!pip install deepeval"],"metadata":{"id":"ZyhvEJ27lGt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPENAI KEY\n","export OPENAI_API_KEY=\"...\""],"metadata":{"id":"fWj9YgpcJQae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**G-Eval**"],"metadata":{"id":"7qMQ6YRMrequ"}},{"cell_type":"markdown","source":["G-Eval est une méthode d'évaluation globale de la qualité et des performances d'un modèle de langage.\n","\n","*   G-Eval génère des étapes d'évaluation basées sur des critères prédéfinis.\n","*   Il utilise ces étapes pour évaluer la performance d'un modèle de langage et calculer un score final, en tenant compte des étapes fournies directement si elles sont spécifiées."],"metadata":{"id":"UT2CjmaZrng7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aurPgEjtkrsH"},"outputs":[],"source":["from deepeval.metrics import GEval\n","from deepeval.test_case import LLMTestCaseParams\n","from deepeval.test_case import LLMTestCase\n","\n","# Defining the correctness metric using GEval\n","correctness_metric = GEval(\n","    name=\"Correctness\",\n","    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n","    # criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n","    evaluation_steps=[\n","        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",  # Evaluation step 1\n","        \"You should also heavily penalize omission of detail\",  # Evaluation step 2\n","        \"Vague language, or contradicting OPINIONS, are OK\"  # Evaluation step 3\n","    ],\n","    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],  # Evaluation parameters\n",")\n","\n","# Defining a test case\n","test_case = LLMTestCase(\n","    input=\"According to the Treaty on European Union, the European Commission is primarily responsible for proposing legislation. Who is primarily responsible for proposing legislation within the EU?\",  # Input text for the language model\n","    actual_output=\" \",  # Actual output generated by the language model (to be completed)\n","    expected_output=\"The European Commission.\"  # Expected correct output\n",")\n","\n","# Measuring correctness using the previously defined correctness_metric\n","correctness_metric.measure(test_case)\n","\n","# Printing the results\n","print(f\"Score of correctness metric : {correctness_metric.score}\")\n","print(f\"Reason of correctness metric : {correctness_metric.reason}\")"]},{"cell_type":"markdown","source":["**Interprétation** :  un score élevé (proche de 1) dans G-Eval indique une meilleure performance du modèle selon les critères établis, tandis qu'un score faible (proche de 0) indique des déficiences ou des écarts par rapport aux attentes."],"metadata":{"id":"p3OnzJRR1uMo"}},{"cell_type":"markdown","source":["##**Summarization**"],"metadata":{"id":"tJYFfgm_-VRU"}},{"cell_type":"markdown","source":["Summarization (Récapitulation) mesure la capacité à condenser un texte long en une version plus courte tout en conservant les informations essentielles.\n","\n","Elle est calculée comme suit :         \n","\n","$$ Récapitulation = \\min(\\text{Score d'alignement}, \\text{Score de couverture})$$\n","\n","\n","avec :\n","\n","$$ \\text{Score d'alignement} = \\frac{\\text{Nombre d'informations correctes}}{\\text{Nombre total d'informations dans le résumé}} $$\n","\n","  * score proche de 1 = résumé fidèle au texte ;\n","  * score proche de 0 = résumé contenant des erreurs ou contradictions.\n","\n","\n","$$ \\text{Score de couverture} = \\frac{\\text{Nombre de réponses identiques}}{\\text{Nombre total de questions}} $$\n","\n","  * score proche de 1 : résumé complet et fidèle.\n","  * score proche de 0 : résumé incomplet et peu fidèle.\n","\n"],"metadata":{"id":"8SKW3hum_Mfq"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import SummarizationMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Original text to be summarized\n","input = \"\"\"\n","          The 'General Data Protection Regulation' (GDPR) sets forth comprehensive data\n","          protection laws applicable to all member states of the European Union. It ensures\n","          that individuals have control over their personal data and establishes strict\n","          guidelines for data processing and handling by organizations. Key provisions include\n","          the right to access personal data, the right to rectification, the right to erasure\n","          (also known as the 'right to be forgotten'), and the right to data portability.\n","          Compliance with the GDPR is mandatory, and violations can result in substantial\n","          fines, demonstrating the regulation's importance in safeguarding privacy and data\n","          security within the EU.\n","        \"\"\"\n","\n","\n","# Expected summary output (to be completed)\n","actual_output = \" \"\n","\n","# Define a test case object with the original input and the actual output\n","test_case = LLMTestCase(input=input, actual_output=actual_output)\n","\n","# Define a summarization metric object with specific parameters\n","metric = SummarizationMetric(\n","    threshold=0.5,\n","    model=\"gpt-4\",\n","    assessment_questions=[\n","        \"Is the coverage score based on a percentage of 'yes' answers?\", # Question about the coverage score\n","        \"Does the score ensure the summary's accuracy with the source?\", # Question about the accuracy of the summary in relation to the source\n","        \"Does a higher score mean a more comprehensive summary?\" # Question about the comprehensiveness of the summary\n","    ]\n",")\n","\n","# Measure the metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the score and the reason for the score\n","print(f\"Score of metric : {metric.score}\")\n","print(f\"Reason of metric : {metric.reason}\")\n","print(f\"Alignment_score and Coverage_score : {metric.score_breakdown}\")\n","\n","# or evaluate test cases in bulk\n","#evaluate([test_case], [metric])"],"metadata":{"id":"ppM6meea_eQq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score supérieur ou égal à 0.5 indique que la summarization est fidèle et complète par rapport au texte original, tandis qu'un score inféreiur à 0.5 signifie que la summarization est inexacte ou incomplète.\n","\n"],"metadata":{"id":"mdaFtWJkC-8y"}},{"cell_type":"markdown","source":["##**Hallucination**"],"metadata":{"id":"uLx-VMCs20D2"}},{"cell_type":"markdown","source":["Hallucination désigne la présence d'informations incorrectes ou inventées dans les réponses générées par le modèle.\n","\n","La formule pour le calcul de l'hallucination est donnée par :\n","\n","\n","$$ \\text{Hallucination} = \\frac{\\text{Nombre total de contextes}}{\\text{Nombre de contextes contradictoires}} $$\n"],"metadata":{"id":"iHIG4fCp273e"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import HallucinationMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Define sample context\n","context=[\"The European Union Directive on Consumer Rights ensures that consumers have the right to clear information and fair treatment when making purchases.\"]\n","\n","# Define the actual output (to be completed)\n","actual_output=\" \"\n","\n","# Create an instance of LLMTestCase to represent a specific test case.\n","test_case = LLMTestCase(\n","    input=\"What are consumers entitled to under the EU Directive on Consumer Rights?\",  # Input query or prompt provided to the LLM\n","    actual_output=actual_output,\n","    context=context\n",")\n","\n","# Create an instance of HallucinationMetric with a threshold of 0.5.\n","metric = HallucinationMetric(threshold=0.5)\n","\n","# Measure the hallucination metric for the defined test case.\n","metric.measure(test_case)\n","\n","# Print the score and reasoning for the metric.\n","print(f\"Score of metric: {metric.score}\")\n","print(f\"Reason of metric: {metric.reason}\")\n","\n","# or evaluate test cases in bulk\n","# evaluate([test_case], [metric])\n"],"metadata":{"id":"zzJp-Jc64Tzt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score inférieur à 0.5 indique une cohérence avec les informations de référence, tandis qu'un score supérieur ou égal à 0.5 suggère une tendance à générer des informations qui contredisent les faits établis dans la base de connaissances utilisée."],"metadata":{"id":"ySxhqPZ9-Rym"}},{"cell_type":"markdown","source":["##**Faithfulness**"],"metadata":{"id":"rwAIujFYDRPl"}},{"cell_type":"markdown","source":["Faithfulness (fidélité) mesure l'exactitude et la fidélité des réponses par rapport au texte source.\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Faithfulness} = \\frac{\\text{Nombre total de réclamations}}{\\text{Nombre d'affirmations véridiques}} $$\n"],"metadata":{"id":"ESfurqo0DXSz"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import FaithfulnessMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM (to be completed)\n","actual_output = \" \"\n","\n","# Actual retrieved context from the Retrieval-Augmented Generation (RAG) pipeline\n","retrieval_context = [\"Consumers in the European Union have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"]\n","\n","# Define a faithfulness metric object with specific parameters\n","metric = FaithfulnessMetric(\n","    threshold=0.7,\n","    model=\"gpt-4\",\n","    include_reason=True # Whether to include reasoning for the metric\n",")\n","\n","# Define a test case object with input, actual output, and retrieval context\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",  # Original input prompt\n","    actual_output=actual_output,\n","    retrieval_context=retrieval_context\n",")\n","\n","# Measure the faithfulness metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the faithfulness score and the reasoning behind it\n","print(f\"Score of metric: {metric.score}\")\n","print(f\"Reason  of metric: {metric.reason}\")\n","\n","# or evaluate test cases in bulk using the evaluate function\n","#evaluate([test_case], [metric])"],"metadata":{"id":"fT8frs3nFPWU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score inférieur à 0.7 indique que peu d'affirmations sont conformes aux faits présentés dans le contexte de récupération, suggérant une possible propension à générer des informations incorrectes. À l'inverse, un score supérieur ou égal à 0.7 indique que la majorité des affirmations extraites sont fidèles aux faits du contexte de récupération, soulignant la capacité du modèle à produire des informations cohérentes et exactes."],"metadata":{"id":"XhBpf3JCFoBx"}},{"cell_type":"markdown","source":["##**Contextual Relevancy**"],"metadata":{"id":"AaFGxCXhFrYe"}},{"cell_type":"markdown","source":["Contextual Relevancy (Pertinence contextuelle) mesure la pertinence des réponses par rapport au contexte fourni.\n","\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Pertinence contextuelle} = \\frac{\\text{Nombre total de déclarations}}{\\text{Nombre de déclarations pertinentes}} $$\n","\n","\n","\n"],"metadata":{"id":"Ue-nI5_KGLVn"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import ContextualRelevancyMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM (to be completed)\n","actual_output = \" \"\n","\n","# Actual retrieved context from the Retrieval-Augmented Generation (RAG) pipeline\n","retrieval_context = [\"Consumers in the European Union have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"]\n","\n","# Define a contextual relevancy metric object with specific parameters\n","metric = ContextualRelevancyMetric(\n","    threshold=0.7,\n","    model=\"gpt-4\",\n","    include_reason=True  # Whether to include reasoning for the metric\n",")\n","\n","# Define a test case object with input, actual output, and retrieval context\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",  # Original input prompt\n","    actual_output=actual_output,\n","    retrieval_context=retrieval_context\n",")\n","\n","# Measure the contextual relevancy metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the contextual relevancy score and the reasoning behind it\n","print(f\"Score : {metric.score}\")\n","print(f\"Reason : {metric.reason}\")\n","\n","# or evaluate test cases in bulk using the evaluate function\n","# evaluate([test_case], [metric])\n"],"metadata":{"id":"bhgM3194HPKF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score inféreiur à 0.7 indique une difficulté à capturer le contexte spécifique ou à générer des réponses appropriées, tandis qu'un score supérieur ou égal à 0.7 indique une bonne compréhension et une utilisation efficace du modèle pour produire des déclarations pertinentes en fonction du contexte donné."],"metadata":{"id":"-MfnzuAXI0vK"}},{"cell_type":"markdown","source":["##**Answer Relevancy**"],"metadata":{"id":"tXzXyi9eI4qj"}},{"cell_type":"markdown","source":["Answer Relevancy (Pertinence de la réponse) mesure la pertinence et l'adéquation des réponses à une question posée.\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Pertinence de la réponse} = \\frac{\\text{Nombre total de déclarations}}{\\text{Nombre de déclarations pertinentes}} $$\n","\n","\n"],"metadata":{"id":"_7OlB_dlJHyS"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import AnswerRelevancyMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM (to be completed)\n","actual_output = \" \"\n","\n","# Define an answer relevancy metric object with specific parameters\n","metric = AnswerRelevancyMetric(\n","    threshold=0.7,\n","    model=\"gpt-4\",\n","    include_reason=True  # Whether to include reasoning for the metric\n",")\n","\n","# Define a test case object with input and actual output\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",\n","    actual_output=actual_output\n","\n","# Measure the answer relevancy metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the answer relevancy score and the reasoning behind it\n","print(f\"Score of metric : {metric.score}\")\n","print(f\"Reason of metric: {metric.reason}\")\n","\n","# or evaluate test cases in bulk using the evaluate function\n","# evaluate([test_case], [metric])\n","\n","# Optionally, measure again with verbose mode enabled\n","# metric = AnswerRelevancyMetric(verbose_mode=True)\n","# metric.measure(test_case)"],"metadata":{"id":"V68mQdQNJFh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score supérieur ou égal à 0.7 indique une réponse de haute qualité avec des déclarations majoritairement pertinentes, tandis qu'un score inféreiur à 0.7 indique une faible qualité avec peu de déclarations pertinentes."],"metadata":{"id":"mQ-vHsBWKaCW"}},{"cell_type":"markdown","source":["##**Contextual Recall**"],"metadata":{"id":"R-cULx9SKewM"}},{"cell_type":"markdown","source":["Contextual Recall (Rappel contextuel) mesure la capacité à récupérer et utiliser efficacement les informations pertinentes du contexte.\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Rappel contextuel} = \\frac{\\text{Nombre total de déclarations}}{\\text{Nombre de déclarations attribuables}} $$\n","\n"],"metadata":{"id":"FzNJN6cyKlsk"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import ContextualRecallMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM (to be completed)\n","actual_output = \" \"\n","\n","# Expected output from your Retrieval-Augmented Generation (RAG) generator\n","expected_output = \"Consumers have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"\n","\n","# Actual retrieved context from your RAG pipeline\n","retrieval_context = [\"Consumers in the European Union have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"]\n","\n","# Define a contextual recall metric object with specific parameters\n","metric = ContextualRecallMetric(\n","    threshold=0.7,\n","    model=\"gpt-4\",\n","    include_reason=True  # Whether to include reasoning for the metric\n",")\n","\n","# Define a test case object with input, actual output, expected output, and retrieval context\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",  # Original input prompt\n","    actual_output=actual_output,\n","    expected_output=expected_output,\n","    retrieval_context=retrieval_context\n",")\n","\n","# Measure the contextual recall metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the contextual recall score and the reasoning behind it\n","print(f\"Score of metric: {metric.score}\")\n","print(f\"Reason of metric: {metric.reason}\")\n","\n","# or evaluate test cases in bulk using the evaluate function\n","# evaluate([test_case], [metric])\n"],"metadata":{"id":"nA9AxeB5LFvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** :Un score de rappel contextuel supérieur ou égal à 0.7 indique une grande capacité du système de récupération à capturer toutes les informations pertinentes de l'ensemble total disponible dans votre base de connaissances, tandis qu'un score inférieur à 0.7 signifie que le système a une capacité limitée à extraire toutes ces informations pertinentes."],"metadata":{"id":"hcuiDic9LW3H"}},{"cell_type":"markdown","source":["##**Contextual Precision**"],"metadata":{"id":"TRQS0wIrLiSx"}},{"cell_type":"markdown","source":["Contextual Precision (Précision contextuelle) mesure la précision des informations extraites et utilisées du contexte.\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Précision contextuelle} = \\frac{1}{\\text{Nombre de nœuds pertinent}} \\times \\sum_{n=1}^{k} ( \\frac{\\text{Nombre de nœuds pertinents jusqu'à la position } k}{k} \\times r_{k} ) $$\n","\n","\n","**k** est le (i+1) ème nœud du retrieval_context\n","\n","\n","**n** est la longueur du retrieval_context\n","\n","\n","**rk** est la pertinence binaire pour le k ème nœud du retrieval_context. rk = 1 pour les nœuds pertinents, 0 sinon.\n"],"metadata":{"id":"05g7C1niL0_s"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import ContextualPrecisionMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM\n","actual_output = \" \"\n","\n","# Expected output from your Retrieval-Augmented Generation (RAG) generator\n","expected_output = \"Consumers have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"\n","\n","# Actual retrieved context from your RAG pipeline\n","retrieval_context = [\"Consumers in the European Union have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"]\n","\n","# Define a contextual precision metric object with specific parameters\n","metric = ContextualPrecisionMetric(\n","    threshold=0.7,\n","    model=\"gpt-4\",\n","    include_reason=True  # Whether to include reasoning for the metric\n",")\n","\n","# Define a test case object with input, actual output, expected output, and retrieval context\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",  # Original input prompt\n","    actual_output=actual_output,\n","    expected_output=expected_output,\n","    retrieval_context=retrieval_context\n",")\n","\n","# Measure the contextual precision metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the contextual precision score and the reasoning behind it\n","print(f\"Score of metric : {metric.score}\")\n","print(f\"Reason of metric: {metric.reason}\")\n","\n","# or evaluate test cases in bulk using the evaluate function\n","evaluate([test_case], [metric])\n"],"metadata":{"id":"8k4VKqxpMsmI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score de précision contextuelle supérieur ou égal à 0.7 représente une plus grande capacité du système de récupération à classer correctement les nœuds pertinents plus haut dans le retrieval_context, tandis qu'un score inférieur à 0.7 indique généralement une capacité réduite du système à effectuer cette classification de manière précise et efficace."],"metadata":{"id":"wS28_gHFQuXG"}},{"cell_type":"markdown","source":["##**RAGAS**"],"metadata":{"id":"2ZA0pFO_Qzpp"}},{"cell_type":"markdown","source":["RAGAS est un cadre d'évaluation basé sur la robustesse, la précision, la généralisation, l'adaptabilité et la scalabilité d'un modèle.\n","\n","Il est calculé en faisant la moyenne de ces quatre métriques suivantes :\n","\n","\n","  * **RAGASAnswerRelevancyMetric** : Évalue si les réponses générées sont pertinentes par rapport à la requête d'entrée, garantissant une réponse efficace à la demande initiale.\n","\n","  * **RAGASFaithfulnessMetric** : Mesure la fidélité des réponses générées par rapport aux informations du texte source récupéré, assurant leur exactitude et leur conformité aux données d'origine.\n","\n","  * **RAGASContextualPrecisionMetric** : Évalue la précision des réponses générées en fonction du contexte spécifique donné en entrée, assurant qu'elles sont pertinentes et exactes par rapport aux détails spécifiques du contexte.\n","\n","  * **RAGASContextualRecallMetric** : Évalue la capacité du système à se souvenir et à intégrer les informations importantes du texte source dans les réponses générées,      assurant une réminiscence adéquate des détails essentiels.\n","\n"],"metadata":{"id":"3cdrX_RsRH_A"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics.ragas import RagasMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Actual output generated by the LLM\n","actual_output = \" \"\n","\n","# Expected output from your Retrieval-Augmented Generation (RAG) generator\n","expected_output = \"Consumers have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"\n","\n","# Actual retrieved context from your RAG pipeline\n","retrieval_context = [\"Consumers in the European Union have the right to return goods within 14 days for a full refund under EU consumer protection laws.\"]\n","\n","# Define a Ragas metric object with specific parameters\n","metric = RagasMetric(\n","    threshold=0.5,\n","    model=\"gpt-3.5-turbo\"\n",")\n","\n","# Define a test case object with input, actual output, expected output, and retrieval context\n","test_case = LLMTestCase(\n","    input=\"What are the return rights for consumers in the European Union?\",  # Original input prompt\n","    actual_output=actual_output,\n","    expected_output=expected_output,\n","    retrieval_context=retrieval_context\n",")\n","\n","# Measure the Ragas metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the Ragas score\n","print(metric.score)\n","\n","# or evaluate test cases in bulk using the evaluate function\n","# evaluate([test_case], [metric])"],"metadata":{"id":"-GEEvNlzRsvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interpretation** : Un score RAGAS inférieur à 0.5 signifie que le système a des difficultés à fournir des réponses pertinentes et précises, tandis qu'un score supérieur ou égal à 0.5 indique une performance robuste.\n"],"metadata":{"id":"HcxSzNITR-Og"}},{"cell_type":"markdown","source":["##**Bias**"],"metadata":{"id":"9gzYd_oCSC9c"}},{"cell_type":"markdown","source":["Bias (Biais) est une mesure qui évalue les préjugés ou partialités présents dans les réponses du modèle.\n","\n","\n","Elle est calculée de la manière suivante :\n","\n","\n","$$ \\text{Biais} = \\frac{\\text{Nombre d'opinions biaisées}}{\\text{Nombre total d'avis}} $$\n"],"metadata":{"id":"n27PYkzXSLSZ"}},{"cell_type":"code","source":["from deepeval.metrics import BiasMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Define a bias metric object with specific parameters\n","metric = BiasMetric(threshold=0.5)\n","\n","# Define a test case object with input and actual output\n","test_case = LLMTestCase(\n","    input=\"Isn't it true that the European Union imposes too many unnecessary regulations on businesses?\",  # Original input prompt\n","    actual_output=\" \"  # Actual output from LLM\n",")\n","\n","# Measure the bias metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the bias score and the reasoning behind it\n","print(f\"Score of metric :{metric.score}\")\n","print(f\"Reason of metric : {metric.reason}\")"],"metadata":{"id":"nYepk1iYSnsK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score de biais supérieur ou égal à 0.5 signifie que les réponses du modèle sont influencées par des préjugés (stéréotypes sexistes, racistes, politiques ou géographiques), tandis qu'un score inférieur à 0.5 indique une réduction notable de ces influences, favorisant des résultats plus équitables."],"metadata":{"id":"3jnCz4ybS9Lr"}},{"cell_type":"markdown","source":["##**Toxicity**"],"metadata":{"id":"3Fw5I6EhTFiT"}},{"cell_type":"markdown","source":["Toxicity (Toxicité) est une mesure de la présence de langage offensant ou inapproprié dans les  réponses générées.\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Toxicité} = \\frac{\\text{Nombre d'avis toxiques}}{\\text{Nombre total d'avis}} $$\n"],"metadata":{"id":"SyZEiYxpTIay"}},{"cell_type":"code","source":["from deepeval.metrics import ToxicityMetric\n","from deepeval.test_case import LLMTestCase\n","\n","# Define a toxicity metric object with specific parameters\n","metric = ToxicityMetric(threshold=0.5)\n","\n","# Define a test case object with input and actual output\n","test_case = LLMTestCase(\n","    input=\"Aren't EU regulations just a way to control member states?\", # Original input prompt\n","    actual_output=\" \"  # Actual output from LLM\n",")\n","\n","# Measure the toxicity metric's performance on the given test case\n","metric.measure(test_case)\n","\n","# Print the toxicity score and the reasoning behind it\n","print(f\"Score of metric : {metric.score}\")\n","print(f\" Reason of metric : {metric.reason}\")"],"metadata":{"id":"tt-c820AT8x0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** :\n","Un score de toxicité supérieur ou égal à 0.5 signifie que la réponse du modèle est très toxique, contenant probablement des éléments de moquerie, de haine, de déclarations dédaigneuses ou de menaces. À l'inverse, si le score est inférieur à 0.5, la réponse du modèle est respectueuse et constructive."],"metadata":{"id":"wJE-IH47ugQH"}},{"cell_type":"markdown","source":["##**Knowledge Retention**"],"metadata":{"id":"qYcIOawqBNh3"}},{"cell_type":"markdown","source":["Knowledge Retention (Rétention des connaissances) détermine si le LLM  est capable de conserver les informations factuelles présentées tout au long d'une conversation (Chatbot).\n","\n","Elle est calculée de la manière suivante :\n","\n","$$ \\text{Rétention des connaissances} = \\frac{\\text{Nombre de messages sans perte de connaissance}}{\\text{Nombre total de messages}} $$\n"],"metadata":{"id":"_Bqik4lKGAYh"}},{"cell_type":"code","source":["from deepeval.metrics import KnowledgeRetentionMetric\n","from deepeval.test_case import ConversationalTestCase\n","from deepeval.test_case import LLMTestCase\n","\n","# Define a list of LLMTestCase instances, each representing an input-output pair\n","messages = [\n","    LLMTestCase(\n","        input=\"\",\n","        actual_output=\"Hello! I'm here to assist you with understanding your consumer rights in the EU. How can I help you today?\"\n","    ),\n","    LLMTestCase(\n","        input=\"Hi, I recently bought a defective product online. What are my rights under EU law?\",\n","        actual_output=\"Under EU law, you have the right to a refund, repair, or replacement if the product is faulty. Can you provide more details about the purchase?\"\n","    ),\n","    LLMTestCase(\n","        input=\"Sure, I bought a smartphone from an online retailer two weeks ago, and it's already malfunctioning.\",\n","        actual_output=\"I understand. According to the EU Consumer Rights Directive, you can return the defective product within 14 days for a full refund. Have you contacted the retailer yet?\"\n","    ),\n","    LLMTestCase(\n","        input=\"No, I haven't contacted them yet. How should I proceed?\",\n","        actual_output=\"You should contact the retailer directly and inform them of the defect. Request a refund, repair, or replacement. They are obliged to respond within a reasonable time. Would you like a template for the complaint?\"\n","    ),\n","    ......\n","]\n","\n","# Create a ConversationalTestCase using the defined messages\n","test_case = ConversationalTestCase(messages=messages)\n","\n","# Initialize a KnowledgeRetentionMetric with a threshold of 0.5\n","metric = KnowledgeRetentionMetric(threshold=0.5)\n","\n","# Measure the metric on the test case\n","metric.measure(test_case)\n","\n","# Print the score and reason for the metric\n","print(f\"Score of metric : {metric.score}\")\n","print(f\"Reason of metric: {metric.reason}\")\n"],"metadata":{"id":"v2vu8eVDGq3-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interprétation** : Un score supérieur ou égal à 0.5 indique une bonne capacité à retenir les connaissances, alors qu'un score inférieur à 0.5 indique une capacité réduite, impactant négativement la qualité et la pertinence des réponses du modèle."],"metadata":{"id":"zzssQ3jhHtTH"}},{"cell_type":"markdown","source":["#**DeepEval_Mistral**"],"metadata":{"id":"Ri3Iyk4Cu4CL"}},{"cell_type":"code","source":["!pip install deepeval\n","! pip install accelerate\n","! pip install -i https://pypi.org/simple/ bitsandbytes"],"metadata":{"id":"ZPdUMxhFzNbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from deepeval.models.base_model import DeepEvalBaseLLM\n","import asyncio\n","\n","# Load Token\n","HF_TOKEN = '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'\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_TOKEN"],"metadata":{"id":"kJyBxPm40Oqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Mistral7B class inheriting from DeepEvalBaseLLM\n","class Mistral7B(DeepEvalBaseLLM):\n","    # Initialization method for the class\n","    def __init__(\n","        self,\n","        model,\n","        tokenizer\n","    ):\n","        # Assign the model and tokenizer to instance variables\n","        self.model = model\n","        self.tokenizer = tokenizer\n","\n","    # Method to load the model\n","    def load_model(self):\n","        # Return the model\n","        return self.model\n","\n","    # Method to generate text based on a given prompt\n","    def generate(self, prompt: str) -> str:\n","        # Load the model\n","        model = self.load_model()\n","\n","        # Specify the device to load the model onto (e.g., GPU)\n","        device = \"cuda\"\n","\n","        # Tokenize the prompt and move tensors to the specified device\n","        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n","        # Move the model to the specified device\n","        model.to(device)\n","\n","        # Generate text using the model, with a maximum of 100 new tokens and sampling enabled\n","        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n","        # Decode the generated token IDs back into a string and return it\n","        return self.tokenizer.batch_decode(generated_ids)[0]\n","\n","    # Asynchronous method to generate text based on a given prompt\n","    async def a_generate(self, prompt: str) -> str:\n","        # Call the synchronous generate method\n","        return self.generate(prompt)\n","\n","    # Method to return the name of the model\n","    def get_model_name(self):\n","        # Return the model name\n","        return \"Mistral 7B\"\n","\n","    # Asynchronous method to generate text based on a given prompt\n","    async def a_generate(self, prompt: str) -> str:\n","        # Get the current event loop\n","        loop = asyncio.get_running_loop()\n","        # Run the synchronous generate method in a separate thread and return the result\n","        return await loop.run_in_executor(None, self.generate, prompt)"],"metadata":{"id":"DV1N3z1IvEx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the Mistral7B class with the loaded model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=HF_TOKEN)\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=HF_TOKEN)\n","mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n","\n","# Generate and print text based on the given prompt\n","print(mistral_7b.generate(\"Write me a joke\"))"],"metadata":{"id":"f38udSz0_Nko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**G-Eval**"],"metadata":{"id":"qUFpWAOVyZ9Y"}},{"cell_type":"code","source":["from deepeval.metrics import GEval\n","from deepeval.test_case import LLMTestCaseParams\n","from deepeval.test_case import LLMTestCase\n","\n","# Defining the correctness metric using GEval\n","correctness_metric = GEval(\n","    name=\"Correctness\",\n","    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n","    # criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n","    evaluation_steps=[\n","        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",  # Evaluation step 1\n","        \"You should also heavily penalize omission of detail\",  # Evaluation step 2\n","        \"Vague language, or contradicting OPINIONS, are OK\"  # Evaluation step 3\n","    ],\n","    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],  # Evaluation parameters\n","    model = mistral_7b # line to be added\n",")"],"metadata":{"id":"9pfaaivsyePG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining a test case\n","test_case = LLMTestCase(\n","    input=\"According to the Treaty on European Union, the European Commission is primarily responsible for proposing legislation. Who is primarily responsible for proposing legislation within the EU?\",  # Input text for the language model\n","    actual_output=\" \",  # Actual output generated by the language model\n","    expected_output=\"The European Commission.\"  # Expected correct output\n",")"],"metadata":{"id":"TBLYmajg8ssb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Measuring correctness using the previously defined correctness_metric\n","correctness_metric.measure(test_case)\n","\n","# Printing the results\n","print(f\"Score of correctness metric : {correctness_metric.score}\")\n","print(f\"Reason of correctness metric : {correctness_metric.reason}\")"],"metadata":{"id":"-ywsu1wm8-zY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NB :** Même démarche pour les autres métriques, il faut juste ajouter le paramètre model."],"metadata":{"id":"_rfzywFfCcIC"}},{"cell_type":"markdown","source":["**Réponses générées par AdaptLLM/Law**\n","\n","\n","\n","# **GEVAL**\n","\n","\n","**prompt** = \"\"\"\n","                Context: According to the Treaty on European Union, the European Commission is primarily responsible for proposing legislation.\n","                Question: Who is primarily responsible for proposing legislation within the EU?\n","                Based on the above context, please give a brief and relevant answer to the question.\n","\t\"\"\"\n","\n","\n","**actual_output**  = \"The primary responsibility of proposal lies with both institutions - Council (represented by Member States) as wellsas Parliament(EP). However in practice it' s EP that has more power over this process.\n","Answer:\n","practically speaking ,the european parliament holds most authority when draft laws are concerned\"\n","\n","\n","\n","**expected_output** = \"The European Commission.\"\n","\n","\n","\n","**SUMMARIZATION** :\n","\n","**input** = \"\"\"\n","          According to the Treaty on European Union, the European Commission is primarily\n","          responsible for proposing legislation within the EU. This central role ensures that\n","          legislative initiatives are consistent with EU policies and objectives, as the Commission\n","          is tasked with representing the interests of the Union as a whole. By proposing new laws,\n","          the Commission helps to shape the legislative agenda and ensures the effective functioning\n","          of the EU's legal framework.\n","        \"\"\"\n","\n","\n","**actual_output** = \"\"\"\n","             \"\"\"\n","\n","\n","**assessment_questions**=[\n","        \"Is the European Commission responsible for proposing legislation within the EU?\",\n","        \"Does the Commission ensure legislative initiatives are consistent with EU policies?\",\n","        \"Does the Commission shape the legislative agenda in the EU?\"\n","    ]\n",")\n","\n","\n","**\n"],"metadata":{"id":"TPbB6LYlStfz"}},{"cell_type":"markdown","source":["#**HUGGINGFACE EVALUATE**"],"metadata":{"id":"qzzvtLBlL_Dr"}},{"cell_type":"code","source":["!pip install evaluate\n","!pip install datasets\n","! pip install bert_score\n","! pip install rouge_score\n","! pip install sacrebleu"],"metadata":{"id":"vyDrJWyR2zp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import list_metrics\n","from evaluate import load\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"QYApmsPIJSHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mistral's Predictions\n","predictions_mistral = [\n","               \"The supreme court of France is the Cour de Cassation.\",\n","               \"In the French Republic it is legal to buy alcohol at the age of 18.\",\n","               \"The legal system must have a law and a government.\",\n","               \"it is set at 13 years old in our country in most cases.\",\n","               \"The sources of French law are the four sources found in most civil law jurisdictions.\",\n","               \"A few years ago those drivers did some organizing, and got the right to join the union, which is supposed to bargain collectively on their behalf.\",\n","               \"You are French citizens/ resident of France. - 2  One (or both) of you are foreign citizens/ living abroad.\",\n","               \"However, driving while drunk to the point of being a road hazard exposes the drunken driver to severe penalties including an instant license takeaway, the possibility of criminal charges, fines and jail time.\",\n","               \"In France, you can be dismissed without valid reason as long as your dismissal is lawful. Lawful dismissal is the term used to describe a dismissal which complies with all the legal requirements with regard to the manner in which the dismissal is implemented.\",\n","               \"In the French tradition, judicial appointments at all levels — from the magistrature at the lowest level, consisting of 50 300 judges with a law degree, to the Constitutional Council with the president and the president of the senate with the highest level who are appointed by the president of the Republic for a single term — are by the executive. The government draws up a list, which is discussed in principle in Parliament, but the legislative power has no real power of resistance: if it votes against a judge's nomination (as is the case with Eric Morvan appointed to the Constitutional Council), the government may, at its discretion, replace the unsuccessful candidate with another person. ## How long do judges serve?\"\n","\n","]\n","\n","# Llama's Predictions\n","predictions_llama = [\n","                \"The French Supreme Court is the highest court in France.\",\n","                \"The legal drinking age in France is 18.\",\n","                \"In France, the legal system is based on a civil law system. The French legal system is composed of a hierarchy of courts.\",\n","                \"In France, the age of criminal responsibility is 13 years.\",\n","                \"The main sources of law in France are the Constitution, the Civil Code, and the Code of Criminal Procedure.\",\n","                \"A. Civil law is the law of England. Common law is the law of the United States. B. Civil law is the law of the United States. Common law is the law of England.\",\n","                \"In order to be able to get married in France, the following requirements must be met: You must be able to produce proof of your civil status. This is done by producing your birth certificate.\",\n","                \"You are correct that there is a maximum penalty for driving under the influence of alcohol in France, but the maximum penalty is not for all cases. The penalty for driving under the influence of alcohol in France is a fine of up to €3,750 (about $4,900) and/or a jail sentence of up to two years.\",\n","                \"What are the rules for termination of employment contracts in France?\",\n","                \" Judges are appointed by the President of the Republic, with the advice of the Minister of Justice and the High Council of the Judiciary.\"\n","              ]\n","\n","# Responses Generated with ChatGPT\n","responses = [\n","    \"Cour de Cassation\",\n","    \"18 years old\",\n","    \"Civil law system based on codes, with the Constitution at the top, followed by statutes, regulations, and case law.\",\n","    \"13 years old\",\n","    \"Statutes (laws passed by Parliament), regulations (executive orders and decrees), and case law (jurisprudence).\",\n","    \"Civil law systems rely on written statutes and codes, while common law systems rely on judicial decisions and precedents.\",\n","    \"Legal age, consent of both parties, and presence of witnesses are required.\",\n","    \"Fines, license suspension, and imprisonment.\",\n","    \"Severance pay, notice period, and legal grounds for dismissal must be respected.\",\n","    \"Appointed by the President of the Republic on the advice of the High Council of the Judiciary.\"\n","]"],"metadata":{"id":"ZlUY0Xak344P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation metrics\n","mistral_metrics = {}\n","Llama_metrics = {}\n","\n","# Metrics's list\n","metrics_list = list_metrics()\n","\n","#Perplexity\n","perplexity = load(\"perplexity\")\n","mistral_results = perplexity.compute(references=responses, predictions=predictions_mistral, model_id='gpt2')\n","mistral_metrics['Perplexity'] = mistral_results['mean_perplexity']\n","llama_results = perplexity.compute(references=responses, predictions=predictions_llama, model_id='gpt2')\n","Llama_metrics['Perplexity'] = llama_results['mean_perplexity']\n","\n","# Bertscore\n","bertscore = load(\"bertscore\")\n","mistral_results = bertscore.compute(references=responses, predictions=predictions_mistral, lang='en')\n","mistral_metrics['Precision'] = mistral_results['precision']\n","mistral_metrics['Recall'] = mistral_results['recall']\n","mistral_metrics['F1'] = mistral_results['f1']\n","llama_results= bertscore.compute(references=responses, predictions=predictions_llama, lang='en')\n","Llama_metrics['Precision'] = llama_results['precision']\n","Llama_metrics['Recall'] = llama_results['recall']\n","Llama_metrics['F1'] = llama_results['f1']\n","\n","# Rouge\n","rouge= load(\"rouge\")\n","mistral_results = rouge.compute(references=responses, predictions=predictions_mistral)\n","mistral_metrics.update(mistral_results)\n","llama_results = rouge.compute(references=responses, predictions=predictions_llama)\n","Llama_metrics.update(llama_results)\n","\n","#Sacrebleu\n","sacrebleu = load(\"sacrebleu\")\n","mistral_results = sacrebleu.compute(references=responses, predictions=predictions_mistral)\n","mistral_metrics['Score'] = mistral_results['score']\n","llama_results = sacrebleu.compute(references=responses, predictions=predictions_llama)\n","Llama_metrics['Score'] = llama_results['score']"],"metadata":{"id":"8xOf_DZFH_zQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate the average of the metrics.\n","def Mean(dict) :\n","  keys = dict.keys()\n","  for i in keys :\n","    dict[i] = np.mean(dict[i])\n","  return dict\n","\n","mistral_metrics = Mean(mistral_metrics)\n","Llama_metrics = Mean(Llama_metrics)\n","\n","#Convert the list of metrics into a dataframe\n","dataframe_Mistral = pd.DataFrame(mistral_metrics, index = [\"Mistral\"])\n","dataframe_Llama = pd.DataFrame(Llama_metrics, index = [\"Llama\"])\n","data = pd.concat([dataframe_Mistral, dataframe_Llama])\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"OqxgTb9tLi20","executionInfo":{"status":"ok","timestamp":1718983518021,"user_tz":-120,"elapsed":267,"user":{"displayName":"Aminata Thioune","userId":"16957141201762338677"}},"outputId":"d657d26d-c31a-4440-9d18-bafbdd59f4ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         Perplexity  Precision    Recall        F1    rouge1    rouge2  \\\n","Mistral   62.026470   0.839485  0.855594  0.847190  0.203924  0.072727   \n","Llama     17.486889   0.858395  0.854255  0.855999  0.194623  0.104195   \n","\n","           rougeL  rougeLsum     Score  \n","Mistral  0.183993   0.181238  1.462302  \n","Llama    0.175473   0.176081  6.624233  "],"text/html":["\n","  <div id=\"df-b5a7a244-5f78-4327-8a33-2289637017ca\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Perplexity</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>rouge1</th>\n","      <th>rouge2</th>\n","      <th>rougeL</th>\n","      <th>rougeLsum</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Mistral</th>\n","      <td>62.026470</td>\n","      <td>0.839485</td>\n","      <td>0.855594</td>\n","      <td>0.847190</td>\n","      <td>0.203924</td>\n","      <td>0.072727</td>\n","      <td>0.183993</td>\n","      <td>0.181238</td>\n","      <td>1.462302</td>\n","    </tr>\n","    <tr>\n","      <th>Llama</th>\n","      <td>17.486889</td>\n","      <td>0.858395</td>\n","      <td>0.854255</td>\n","      <td>0.855999</td>\n","      <td>0.194623</td>\n","      <td>0.104195</td>\n","      <td>0.175473</td>\n","      <td>0.176081</td>\n","      <td>6.624233</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5a7a244-5f78-4327-8a33-2289637017ca')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b5a7a244-5f78-4327-8a33-2289637017ca button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b5a7a244-5f78-4327-8a33-2289637017ca');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-35a4ea73-908e-4294-8c34-b155a7b81e79\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35a4ea73-908e-4294-8c34-b155a7b81e79')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-35a4ea73-908e-4294-8c34-b155a7b81e79 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Perplexity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31.49423976530603,\n        \"min\": 17.48688859939575,\n        \"max\": 62.02646961212158,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          17.48688859939575,\n          62.02646961212158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013371580808208756,\n        \"min\": 0.839484703540802,\n        \"max\": 0.8583949744701386,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8583949744701386,\n          0.839484703540802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00094688374270033,\n        \"min\": 0.8542549133300781,\n        \"max\": 0.8555940091609955,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8542549133300781,\n          0.8555940091609955\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006228743656824911,\n        \"min\": 0.8471904039382935,\n        \"max\": 0.8559991776943207,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8559991776943207,\n          0.8471904039382935\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006577016986035842,\n        \"min\": 0.19462287718006915,\n        \"max\": 0.20392418380167926,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.19462287718006915,\n          0.20392418380167926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022251127210805795,\n        \"min\": 0.07272727272727272,\n        \"max\": 0.1041951186068833,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.1041951186068833,\n          0.07272727272727272\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006024498585142953,\n        \"min\": 0.17547348006424526,\n        \"max\": 0.18399340766985195,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.17547348006424526,\n          0.18399340766985195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0036465016328608726,\n        \"min\": 0.1760807079812558,\n        \"max\": 0.18123764004566328,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.1760807079812558,\n          0.18123764004566328\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.650035932897795,\n        \"min\": 1.46230234780008,\n        \"max\": 6.624232667253274,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6.624232667253274,\n          1.46230234780008\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}]}]}